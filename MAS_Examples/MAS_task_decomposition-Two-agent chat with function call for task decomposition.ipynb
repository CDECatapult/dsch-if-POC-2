{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {
    "id": "9a71fa36"
   },
   "source": [
    "# Task Decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7fee7c-a1bc-49e0-8086-87c3a85d4871",
   "metadata": {},
   "source": [
    "## The Approach1. Two-agent chat with function call for task decomposition\n",
    "\n",
    "In this approach, we use a planner agent for coming up a plan.\n",
    "The planner agent is wrapped inside a function to be used as a tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c436c9",
   "metadata": {
    "id": "a6c436c9"
   },
   "source": [
    "First make sure the `pyautogen` package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659cd58d",
   "metadata": {
    "id": "659cd58d"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq \"pyautogen>=0.2.18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca301a4",
   "metadata": {
    "id": "dca301a4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Callable, Dict, Literal, Optional, Union\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from autogen import (\n",
    "    Agent,\n",
    "    AssistantAgent,\n",
    "    ConversableAgent,\n",
    "    GroupChat,\n",
    "    GroupChatManager,\n",
    "    UserProxyAgent,\n",
    "    config_list_from_json,\n",
    "    register_function,\n",
    ")\n",
    "from autogen.agentchat.contrib import agent_builder\n",
    "from autogen.cache import Cache\n",
    "from autogen.coding import DockerCommandLineCodeExecutor, LocalCommandLineCodeExecutor\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4-1106-preview\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "    {\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "]\n",
    "\n",
    "# You can also use the following method to load the config list from a file or environment variable.\n",
    "# config\n",
    "\n",
    "# config_list=[\n",
    "#         {\n",
    "#             \"model\": \"NotRequired\", # Loaded with LiteLLM command\n",
    "#             \"api_key\": \"NotRequired\", # Not needed\n",
    "#             \"base_url\": \"http://52.56.167.144:4000\",  # Your LiteLLM URL\n",
    "#             \"price\" : [0, 0]\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "llm_config={\n",
    "    \"config_list\": config_list,\n",
    "    \"cache_seed\": None, # Turns off caching, useful for testing different models\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11489237",
   "metadata": {
    "id": "11489237"
   },
   "source": [
    "The task to be solved to write a blog post about the stock price performance of Nvidia in the past month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ecbc602",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ecbc602",
    "outputId": "ccf6a473-ab31-4900-a893-1a1ad4fa4033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The task is to create a python code that translate from  source data model to target data model. \n",
      "\n",
      "An example of source and target data structures are below:\n",
      "source model example in json format:\n",
      "    {\n",
      "        \"volume\":177.0730798,\n",
      "        \"area\":434.4258052,\n",
      "        \"length\":0.0,\n",
      "        \"VolumeUnit\":\"m3\",\n",
      "        \"Category\":\"CV-CV-Abutment-G-P\",\n",
      "        \"ss_epd_id\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "        \"ss_category_name\":\"ReadyMix\",\n",
      "        \"Embodied_Carbon\":54553.4986645559\n",
      "    }\n",
      "\n",
      "target model example translated from the example of source data model in json format:\n",
      "    {\n",
      "        \"Name_notes\":\"CV-CV-Abutment-G-P\",\n",
      "        \"Asset_Code\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "        \"Quantity\":177.0730798,\n",
      "        \"Unit\":\"m3\",\n",
      "        \"Total\":54553.4986645559,\n",
      "        \"Concrete\":54553.4986645559\n",
      "    }\n",
      "\n",
      "another example:\n",
      "source model example in json format:\n",
      "source data model:\n",
      "    {\n",
      "        \"volume\":1.42005247,\n",
      "        \"area\":74.7313579,\n",
      "        \"length\":0.0,\n",
      "        \"VolumeUnit\":\"m3\",\n",
      "        \"Category\":\"CV-CV-Barrier-G-P\",\n",
      "        \"QTY\":110,\n",
      "        \"ss_epd_id\":\"5e99ea4bf1124b66a0899c4e072550f4\",\n",
      "        \"ss_category_name\":\"Steel\",\n",
      "        \"Embodied_Carbon\":6780.3888439634\n",
      "    }\n",
      "target model example translated from the example of source data model in json format:\n",
      "    {\n",
      "        \"Name_notes\":\"CV-CV-Barrier-G-P\",\n",
      "        \"Asset_Code\":\"5e99ea4bf1124b66a0899c4e072550f4\",\n",
      "        \"Quantity\":1.42005247,\n",
      "        \"Unit\":\"m3\",\n",
      "        \"Total\":6780.3888439634,\n",
      "        \"Steel\":6780.3888439634,\n",
      "    }\n",
      "\n",
      "Here is the source model to be translated:\n",
      "\n",
      "    {\n",
      "        \"volume\":177.0730798,\n",
      "        \"area\":434.4258052,\n",
      "        \"length\":0.0,\n",
      "        \"VolumeUnit\":\"m3\",\n",
      "        \"Category\":\"CV-CV-Abutment-G-P\",\n",
      "        \"ss_epd_id\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "        \"ss_category_name\":\"ReadyMix\",\n",
      "        \"Embodied_Carbon\":54553.4986645559\n",
      "    }\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = (\n",
    "    \"\"\"\n",
    "The task is to create a python code that translate from  source data model to target data model.\n",
    "\n",
    "An example of source and target data structures are below:\n",
    "source model example in json format:\n",
    "    {\n",
    "        \"volume\":177.0730798,\n",
    "        \"area\":434.4258052,\n",
    "        \"length\":0.0,\n",
    "        \"VolumeUnit\":\"m3\",\n",
    "        \"Category\":\"CV-CV-Abutment-G-P\",\n",
    "        \"ss_epd_id\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
    "        \"ss_category_name\":\"ReadyMix\",\n",
    "        \"Embodied_Carbon\":54553.4986645559\n",
    "    }\n",
    "\n",
    "target model example translated from the example of source data model in json format:\n",
    "    {\n",
    "        \"Name_notes\":\"CV-CV-Abutment-G-P\",\n",
    "        \"Asset_Code\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
    "        \"Quantity\":177.0730798,\n",
    "        \"Unit\":\"m3\",\n",
    "        \"Total\":54553.4986645559,\n",
    "        \"Concrete\":54553.4986645559\n",
    "    }\n",
    "\n",
    "another example:\n",
    "source model example in json format:\n",
    "source data model:\n",
    "    {\n",
    "        \"volume\":1.42005247,\n",
    "        \"area\":74.7313579,\n",
    "        \"length\":0.0,\n",
    "        \"VolumeUnit\":\"m3\",\n",
    "        \"Category\":\"CV-CV-Barrier-G-P\",\n",
    "        \"QTY\":110,\n",
    "        \"ss_epd_id\":\"5e99ea4bf1124b66a0899c4e072550f4\",\n",
    "        \"ss_category_name\":\"Steel\",\n",
    "        \"Embodied_Carbon\":6780.3888439634\n",
    "    }\n",
    "target model example translated from the example of source data model in json format:\n",
    "    {\n",
    "        \"Name_notes\":\"CV-CV-Barrier-G-P\",\n",
    "        \"Asset_Code\":\"5e99ea4bf1124b66a0899c4e072550f4\",\n",
    "        \"Quantity\":1.42005247,\n",
    "        \"Unit\":\"m3\",\n",
    "        \"Total\":6780.3888439634,\n",
    "        \"Steel\":6780.3888439634,\n",
    "    }\n",
    "\n",
    "Here is the source model to be translated:\n",
    "\n",
    "    {\n",
    "        \"volume\":177.0730798,\n",
    "        \"area\":434.4258052,\n",
    "        \"length\":0.0,\n",
    "        \"VolumeUnit\":\"m3\",\n",
    "        \"Category\":\"CV-CV-Abutment-G-P\",\n",
    "        \"ss_epd_id\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
    "        \"ss_category_name\":\"ReadyMix\",\n",
    "        \"Embodied_Carbon\":54553.4986645559\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54823657",
   "metadata": {
    "id": "54823657"
   },
   "outputs": [],
   "source": [
    "# Create planner agent.\n",
    "planner = AssistantAgent(\n",
    "    name=\"planner\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a helpful AI assistant. You suggest a feasible plan \"\n",
    "    \"for finishing a complex task by decomposing it into 3-5 sub-tasks. \"\n",
    "    \"If the plan is not good, suggest a better plan. \"\n",
    "    \"If the execution is wrong, analyze the error and suggest a fix.\",\n",
    ")\n",
    "\n",
    "# Create a planner user agent used to interact with the planner.\n",
    "planner_user = UserProxyAgent(\n",
    "    name=\"planner_user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "# The function for asking the planner.\n",
    "\n",
    "\n",
    "def task_planner(question: Annotated[str, \"Question to ask the planner.\"]) -> str:\n",
    "    with Cache.disk(cache_seed=4) as cache:\n",
    "        planner_user.initiate_chat(planner, message=question, max_turns=1, cache=cache)\n",
    "    # return the last message received from the planner\n",
    "    return planner_user.last_message()[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a024e2",
   "metadata": {
    "id": "11a024e2"
   },
   "source": [
    "Next, we create an assistant agent to execute the plan, using the planner agent as a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fb85afb",
   "metadata": {
    "id": "9fb85afb"
   },
   "outputs": [],
   "source": [
    "# Create assistant agent.\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can use the task planner to decompose a complex task into sub-tasks. \"\n",
    "    \"Make sure your follow through the sub-tasks. \"\n",
    "    \"When needed, write Python code in markdown blocks, and I will execute them.\"\n",
    "    \"If a target field depends on the value of a source field, create a lookup table\"\n",
    "    \"Give the user a final solution at the end. \"\n",
    "    \"Return TERMINATE only if the sub-tasks are completed.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# Setting up code executor.\n",
    "os.makedirs(\"planning\", exist_ok=True)\n",
    "# Use DockerCommandLineCodeExecutor to run code in a docker container.\n",
    "# code_executor = DockerCommandLineCodeExecutor(work_dir=\"planning\")\n",
    "code_executor = LocalCommandLineCodeExecutor(work_dir=\"planning\")\n",
    "\n",
    "# Create user proxy agent used to interact with the assistant.\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    is_termination_msg=lambda x: \"content\" in x\n",
    "    and x[\"content\"] is not None\n",
    "    and x[\"content\"].rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\"executor\": code_executor},\n",
    ")\n",
    "\n",
    "# Register the function to the agent pair.\n",
    "register_function(\n",
    "    task_planner,\n",
    "    caller=assistant,\n",
    "    executor=user_proxy,\n",
    "    name=\"task_planner\",\n",
    "    description=\"A task planner than can help you with decomposing a complex task into sub-tasks.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b44e096",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b44e096",
    "outputId": "e3fcb95d-5add-4072-f74e-47f3e96b335c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_proxy (to assistant):\n",
      "\n",
      "\n",
      "The task is to create a python code that translate from  source data model to target data model. \n",
      "\n",
      "An example of source and target data structures are below:\n",
      "source model example in json format:\n",
      "    {\n",
      "        \"volume\":177.0730798,\n",
      "        \"area\":434.4258052,\n",
      "        \"length\":0.0,\n",
      "        \"VolumeUnit\":\"m3\",\n",
      "        \"Category\":\"CV-CV-Abutment-G-P\",\n",
      "        \"ss_epd_id\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "        \"ss_category_name\":\"ReadyMix\",\n",
      "        \"Embodied_Carbon\":54553.4986645559\n",
      "    }\n",
      "\n",
      "target model example translated from the example of source data model in json format:\n",
      "    {\n",
      "        \"Name_notes\":\"CV-CV-Abutment-G-P\",\n",
      "        \"Asset_Code\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "        \"Quantity\":177.0730798,\n",
      "        \"Unit\":\"m3\",\n",
      "        \"Total\":54553.4986645559,\n",
      "        \"Concrete\":54553.4986645559\n",
      "    }\n",
      "\n",
      "another example:\n",
      "source model example in json format:\n",
      "source data model:\n",
      "    {\n",
      "        \"volume\":1.42005247,\n",
      "        \"area\":74.7313579,\n",
      "        \"length\":0.0,\n",
      "        \"VolumeUnit\":\"m3\",\n",
      "        \"Category\":\"CV-CV-Barrier-G-P\",\n",
      "        \"QTY\":110,\n",
      "        \"ss_epd_id\":\"5e99ea4bf1124b66a0899c4e072550f4\",\n",
      "        \"ss_category_name\":\"Steel\",\n",
      "        \"Embodied_Carbon\":6780.3888439634\n",
      "    }\n",
      "target model example translated from the example of source data model in json format:\n",
      "    {\n",
      "        \"Name_notes\":\"CV-CV-Barrier-G-P\",\n",
      "        \"Asset_Code\":\"5e99ea4bf1124b66a0899c4e072550f4\",\n",
      "        \"Quantity\":1.42005247,\n",
      "        \"Unit\":\"m3\",\n",
      "        \"Total\":6780.3888439634,\n",
      "        \"Steel\":6780.3888439634,\n",
      "    }\n",
      "\n",
      "Here is the source model to be translated:\n",
      "\n",
      "    {\n",
      "        \"volume\":177.0730798,\n",
      "        \"area\":434.4258052,\n",
      "        \"length\":0.0,\n",
      "        \"VolumeUnit\":\"m3\",\n",
      "        \"Category\":\"CV-CV-Abutment-G-P\",\n",
      "        \"ss_epd_id\":\"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "        \"ss_category_name\":\"ReadyMix\",\n",
      "        \"Embodied_Carbon\":54553.4986645559\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "assistant (to user_proxy):\n",
      "\n",
      "***** Suggested tool call (call_8vZDsk5S7djhmwaTSTZJuN9L): task_planner *****\n",
      "Arguments: \n",
      "{\"question\":\"What are the steps needed to translate source data model to target data model?\"}\n",
      "*****************************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
      "\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
      "\n",
      ">>>>>>>> USING AUTO REPLY...\n",
      "\n",
      ">>>>>>>> EXECUTING FUNCTION task_planner...\n",
      "planner_user (to planner):\n",
      "\n",
      "What are the steps needed to translate source data model to target data model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "planner (to planner_user):\n",
      "\n",
      "Translating a source data model to a target data model is a task commonly required in data migrations, data integrations, and system upgrades. It involves converting data from one schema to another while preserving the integrity and meaning of the data. Here are the general steps to approach this task:\n",
      "\n",
      "1. **Understand the Source and Target Data Models:**\n",
      "   - **Sub-task 1:** Analyze the source data model to identify all entities, relationships, constraints, and data types.\n",
      "   - **Sub-tasks 2:** Study the target data model to understand how it differs from the source model in terms of structure, entities, data types, relationships, and constraints.\n",
      "\n",
      "2. **Map the Source to the Target:**\n",
      "   - **Sub-task 1:** Establish mappings between the entities and fields in the source data model and those in the target data model. This should be documented in a mapping specification.\n",
      "   - **Sub-task 2:** Identify any transformations that need to occur for data to fit into the target model. This includes not just field mappings, but also transformations for different data types or formats.\n",
      "   - **Sub-task 3:** Address any data quality issues that may prevent data from fitting into the target model or necessitates data cleansing.\n",
      "\n",
      "3. **Define the Transformation Logic:**\n",
      "   - **Sub-task 1:** Write detailed transformation rules and scripts based on the mapping specification, ensuring that all required data transformations are accounted for.\n",
      "   - **Sub-task 2:** Implement any necessary conversion functions or scripts to handle complex data transformations or to support differences in data types and formats.\n",
      "\n",
      "4. **Testing and Validation:**\n",
      "   - **Sub-task 1:** Develop a set of test cases representing various data scenarios, both normal and edge cases.\n",
      "   - **Sub-task 2:** Run data through the transformation logic and compare the output to expected results detailed in the test cases.\n",
      "   - **Sub-task 3:** Revise transformation rules and scripts as needed based on test outcomes.\n",
      "   - **Sub-task 4:** Perform user acceptance testing to ensure the transformed data meets business requirements and expectations.\n",
      "\n",
      "5. **Execute Data Migration:**\n",
      "   - **Sub-task 1:** Execute the migration process in a controlled environment. This often starts with a subset of data to validate the entire process.\n",
      "   - **Sub-task 2:** If the initial test migration is successful, proceed with the full data migration.\n",
      "   - **Sub-task 3:** Monitor the migration process for errors or issues, addressing them as they arise.\n",
      "\n",
      "6. **Post-Migration Review:**\n",
      "   - **Sub-task 1:** Once the migration is complete, perform data quality checks to ensure no data has been lost or corrupted.\n",
      "   - **Sub-task 2:** Gather feedback from end-users and stakeholders to confirm that the data in the target system is accurate and usable.\n",
      "   - **Sub-task 3:** Document any lessons learned and update processes or mappings for future reference.\n",
      "\n",
      "Each of these steps could be broken down into further detailed sub-tasks depending on the complexity of the data models and the scope of the migration project. This structured approach helps ensure that key aspects of the translation are covered and that the integrity of the data is maintained throughout the process.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:autogen.runtime_logging:[runtime logging] log_function_use: autogen logger is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_proxy (to assistant):\n",
      "\n",
      "user_proxy (to assistant):\n",
      "\n",
      "***** Response from calling tool (call_8vZDsk5S7djhmwaTSTZJuN9L) *****\n",
      "Translating a source data model to a target data model is a task commonly required in data migrations, data integrations, and system upgrades. It involves converting data from one schema to another while preserving the integrity and meaning of the data. Here are the general steps to approach this task:\n",
      "\n",
      "1. **Understand the Source and Target Data Models:**\n",
      "   - **Sub-task 1:** Analyze the source data model to identify all entities, relationships, constraints, and data types.\n",
      "   - **Sub-tasks 2:** Study the target data model to understand how it differs from the source model in terms of structure, entities, data types, relationships, and constraints.\n",
      "\n",
      "2. **Map the Source to the Target:**\n",
      "   - **Sub-task 1:** Establish mappings between the entities and fields in the source data model and those in the target data model. This should be documented in a mapping specification.\n",
      "   - **Sub-task 2:** Identify any transformations that need to occur for data to fit into the target model. This includes not just field mappings, but also transformations for different data types or formats.\n",
      "   - **Sub-task 3:** Address any data quality issues that may prevent data from fitting into the target model or necessitates data cleansing.\n",
      "\n",
      "3. **Define the Transformation Logic:**\n",
      "   - **Sub-task 1:** Write detailed transformation rules and scripts based on the mapping specification, ensuring that all required data transformations are accounted for.\n",
      "   - **Sub-task 2:** Implement any necessary conversion functions or scripts to handle complex data transformations or to support differences in data types and formats.\n",
      "\n",
      "4. **Testing and Validation:**\n",
      "   - **Sub-task 1:** Develop a set of test cases representing various data scenarios, both normal and edge cases.\n",
      "   - **Sub-task 2:** Run data through the transformation logic and compare the output to expected results detailed in the test cases.\n",
      "   - **Sub-task 3:** Revise transformation rules and scripts as needed based on test outcomes.\n",
      "   - **Sub-task 4:** Perform user acceptance testing to ensure the transformed data meets business requirements and expectations.\n",
      "\n",
      "5. **Execute Data Migration:**\n",
      "   - **Sub-task 1:** Execute the migration process in a controlled environment. This often starts with a subset of data to validate the entire process.\n",
      "   - **Sub-task 2:** If the initial test migration is successful, proceed with the full data migration.\n",
      "   - **Sub-task 3:** Monitor the migration process for errors or issues, addressing them as they arise.\n",
      "\n",
      "6. **Post-Migration Review:**\n",
      "   - **Sub-task 1:** Once the migration is complete, perform data quality checks to ensure no data has been lost or corrupted.\n",
      "   - **Sub-task 2:** Gather feedback from end-users and stakeholders to confirm that the data in the target system is accurate and usable.\n",
      "   - **Sub-task 3:** Document any lessons learned and update processes or mappings for future reference.\n",
      "\n",
      "Each of these steps could be broken down into further detailed sub-tasks depending on the complexity of the data models and the scope of the migration project. This structured approach helps ensure that key aspects of the translation are covered and that the integrity of the data is maintained throughout the process.\n",
      "**********************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "assistant (to user_proxy):\n",
      "\n",
      "Based on the provided structure of the source data model and the previous examples, the task of translating the source data model to the target data model seems to include a set of specific mappings and potential value transformations. Here's a breakdown of those steps:\n",
      "\n",
      "1. **Determine the direct mappings from source to target model:**\n",
      "   - Map `Category` to `Name_notes`.\n",
      "   - Map `ss_epd_id` to `Asset_Code`.\n",
      "   - Map `volume` to `Quantity`.\n",
      "   - Map `VolumeUnit` to `Unit`.\n",
      "   - Map `Embodied_Carbon` both to `Total` and the material-specific attribute (e.g., `Concrete` or `Steel`).\n",
      "\n",
      "2. **Identify any required value transformations or lookups:**\n",
      "   - Determine if any values require transformation based on specific business rules (e.g., unit conversion).\n",
      "   - Create a lookup for the material-specific attribute in the target model based on `ss_category_name` from the source model.\n",
      "\n",
      "3. **Develop a Python script to automate the translation process:**\n",
      "   - Write code that reads the source JSON, applies the mappings and transformations, and generates the target JSON format.\n",
      "   - Ensure the script can handle different `ss_category_name` values dynamically by incorporating the lookup table or similar logic.\n",
      "\n",
      "Now, let's create a lookup table based on the provided examples to map the `ss_category_name` to the respective target fields (e.g., `ReadyMix` to `Concrete`, `Steel` to `Steel`). Given the examples, we can infer the following mappings:\n",
      "\n",
      "- `ReadyMix` corresponds to `Concrete`\n",
      "- `Steel` corresponds to `Steel`\n",
      "\n",
      "Please confirm if the lookup table should be based on these inferences or if there are additional rules I need to be aware of for the lookup table.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
      "\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
      "\n",
      ">>>>>>>> USING AUTO REPLY...\n",
      "user_proxy (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "assistant (to user_proxy):\n",
      "\n",
      "Alright, I will proceed with writing a Python script using the direct mappings as described and with a lookup for material-specific attribute based on the `ss_category_name` field. For the given example, we'll use the following mapping for the material attribute:\n",
      "\n",
      "- For `ss_category_name` of \"ReadyMix\", the material attribute in the target model should be `Concrete`.\n",
      "- For `ss_category_name` of \"Steel\", the material attribute in the target model should be `Steel`.\n",
      "\n",
      "Here’s a Python script that should perform the necessary translation:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Source JSON data\n",
      "source_data = {\n",
      "    \"volume\": 177.0730798,\n",
      "    \"area\": 434.4258052,\n",
      "    \"length\": 0.0,\n",
      "    \"VolumeUnit\": \"m3\",\n",
      "    \"Category\": \"CV-CV-Abutment-G-P\",\n",
      "    \"ss_epd_id\": \"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "    \"ss_category_name\": \"ReadyMix\",\n",
      "    \"Embodied_Carbon\": 54553.4986645559\n",
      "}\n",
      "\n",
      "# Lookup for the material-specific attribute in the target model based on ss_category_name\n",
      "material_lookup = {\n",
      "    \"ReadyMix\": \"Concrete\",\n",
      "    \"Steel\": \"Steel\"\n",
      "}\n",
      "\n",
      "# Convert source model to target model\n",
      "def translate_source_to_target(source):\n",
      "    target = {\n",
      "        \"Name_notes\": source[\"Category\"],\n",
      "        \"Asset_Code\": source[\"ss_epd_id\"],\n",
      "        \"Quantity\": source[\"volume\"],\n",
      "        \"Unit\": source[\"VolumeUnit\"],\n",
      "        \"Total\": source[\"Embodied_Carbon\"],\n",
      "    }\n",
      "    \n",
      "    material_key = material_lookup.get(source[\"ss_category_name\"])\n",
      "    if material_key:\n",
      "        target[material_key] = source[\"Embodied_Carbon\"]\n",
      "    \n",
      "    return target\n",
      "\n",
      "# Perform the translation\n",
      "target_data = translate_source_to_target(source_data)\n",
      "\n",
      "# Output the converted target model as JSON\n",
      "print(json.dumps(target_data, indent=4))\n",
      "```\n",
      "\n",
      "This script will transform the provided source model into the intended target model format. It includes logic to handle the specific fields based on the previous examples and the provided source data model.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
      "\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
      "\n",
      ">>>>>>>> USING AUTO REPLY...\n",
      "\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
      "user_proxy (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: {\n",
      "    \"Name_notes\": \"CV-CV-Abutment-G-P\",\n",
      "    \"Asset_Code\": \"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "    \"Quantity\": 177.0730798,\n",
      "    \"Unit\": \"m3\",\n",
      "    \"Total\": 54553.4986645559,\n",
      "    \"Concrete\": 54553.4986645559\n",
      "}\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "assistant (to user_proxy):\n",
      "\n",
      "The Python script has been successfully executed and the source data model has been translated to the target data model as requested. Here's the final translated target data model:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"Name_notes\": \"CV-CV-Abutment-G-P\",\n",
      "    \"Asset_Code\": \"9cf0bb8930ab4d3a9e8082b475796fae\",\n",
      "    \"Quantity\": 177.0730798,\n",
      "    \"Unit\": \"m3\",\n",
      "    \"Total\": 54553.4986645559,\n",
      "    \"Concrete\": 54553.4986645559\n",
      "}\n",
      "```\n",
      "\n",
      "This json shows that the `Category`, `ss_epd_id`, `volume`, `VolumeUnit`, and `Embodied_Carbon` data from the source JSON have been mapped according to the specification to the respective fields in the target model, with the `Embodied_Carbon` being copied to both the `Total` and the `Concrete` fields, in line with the provided `ss_category_name` mapping.\n",
      "\n",
      "If you need to translate more data or make adjustments to the process, feel free to provide more details or additional data models. If this concludes your current request, I'm glad I could assist you!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
     ]
    }
   ],
   "source": [
    "# Use Cache.disk to cache LLM responses. Change cache_seed for different responses.\n",
    "with Cache.disk(cache_seed=1) as cache:\n",
    "    # the assistant receives a message from the user, which contains the task description\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=task,\n",
    "        cache=cache,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
